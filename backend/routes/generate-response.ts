import express, { Request, Response } from 'express';
import { z } from 'zod';
import dotenv from 'dotenv';
import { OpenAI } from 'openai';
import { verifyFirebaseToken } from '../middleware/auth';

dotenv.config();

const router = express.Router();
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const responseSchema = z.object({
  response: z.string(),  // what the AI partner says next
  feedback: z.string(),  // grammar feedback for the learner
});
type ResponseData = z.infer<typeof responseSchema>;

// OPTIMIZATION: Cache for common responses (instant responses!)
const responseCache = new Map<string, ResponseData>([
  ['ã“ã‚“ã«ã¡ã¯', {
    response: 'ã“ã‚“ã«ã¡ã¯ï¼å…ƒæ°—ã§ã™ã‹ï¼Ÿ',
    feedback: 'å®Œç’§ãªæŒ¨æ‹¶ã§ã™ï¼ã€Œå…ƒæ°—ã§ã™ã€ã§ç­”ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚'
  }],
  ['ã“ã‚“ã«ã¡ã¯ã€‚', {
    response: 'ã“ã‚“ã«ã¡ã¯ï¼å…ƒæ°—ã§ã™ã‹ï¼Ÿ',
    feedback: 'å®Œç’§ãªæŒ¨æ‹¶ã§ã™ï¼ã€Œå…ƒæ°—ã§ã™ã€ã§ç­”ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚'
  }],
  ['ã‚ã‚ŠãŒã¨ã†', {
    response: 'ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ã‚‚ã£ã¨ç·´ç¿’ã—ã¾ã—ã‚‡ã†ã€‚',
    feedback: 'ã¨ã¦ã‚‚è‰¯ã„ã§ã™ï¼ä¸å¯§ã«è¨€ã†æ™‚ã¯ã€Œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€ã‚’ä½¿ã„ã¾ã™ã€‚'
  }],
  ['ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™', {
    response: 'ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ã¨ã¦ã‚‚ä¸å¯§ã§ã™ã­ã€‚',
    feedback: 'å®Œç’§ã§ã™ï¼ä¸å¯§ãªè¡¨ç¾ãŒã§ãã¦ã„ã¾ã™ã€‚'
  }],
  ['ãŠã¯ã‚ˆã†', {
    response: 'ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã€‚',
    feedback: 'è‰¯ã„æŒ¨æ‹¶ã§ã™ï¼ä¸å¯§ã«ã€ŒãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€ã¨è¨€ã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚'
  }],
  ['ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™', {
    response: 'ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã¯ä½•ã‚’ã—ã¾ã™ã‹ï¼Ÿ',
    feedback: 'å®Œç’§ãªæœã®æŒ¨æ‹¶ã§ã™ï¼'
  }],
  ['ã“ã‚“ã°ã‚“ã¯', {
    response: 'ã“ã‚“ã°ã‚“ã¯ï¼ä»Šæ—¥ã¯ã©ã†ã§ã—ãŸã‹ï¼Ÿ',
    feedback: 'è‰¯ã„æŒ¨æ‹¶ã§ã™ï¼'
  }],
  ['å…ƒæ°—ã§ã™', {
    response: 'ãã‚Œã¯è‰¯ã‹ã£ãŸã§ã™ï¼ä»Šæ—¥ã¯ä½•ã‚’ã—ã¾ã—ãŸã‹ï¼Ÿ',
    feedback: 'è‰¯ã„ã§ã™ã­ï¼ã€Œå…ƒæ°—ã§ã™ã€ã¯ã€Œã’ã‚“ãã§ã™ã€ã¨èª­ã¿ã¾ã™ã€‚'
  }],
  ['ã¯ã„', {
    response: 'ã‚ã‹ã‚Šã¾ã—ãŸï¼ç¶šã‘ã¾ã—ã‚‡ã†ã€‚',
    feedback: 'ã€Œã¯ã„ã€ã¯ä¸å¯§ãªè¿”äº‹ã§ã™ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã«ã¯ã€Œã†ã‚“ã€ã‚‚ä½¿ãˆã¾ã™ã€‚'
  }],
  ['ã„ã„ãˆ', {
    response: 'ãã†ã§ã™ã‹ã€‚ã§ã¯ã€ä½•ã‹ä»–ã®ã“ã¨ã‚’è©±ã—ã¾ã—ã‚‡ã†ã€‚',
    feedback: 'ã€Œã„ã„ãˆã€ã¯ä¸å¯§ãªå¦å®šã§ã™ã€‚ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã«ã¯ã€Œã†ã†ã‚“ã€ã‚‚ä½¿ãˆã¾ã™ã€‚'
  }],
  ['ã•ã‚ˆã†ãªã‚‰', {
    response: 'ã•ã‚ˆã†ãªã‚‰ï¼ã¾ãŸè©±ã—ã¾ã—ã‚‡ã†ï¼',
    feedback: 'å®Œç’§ã§ã™ï¼ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã«ã¯ã€Œã˜ã‚ƒã‚ã­ã€ã‚„ã€Œã¾ãŸã­ã€ã‚‚ä½¿ãˆã¾ã™ã€‚'
  }],
  ['ã¾ãŸã­', {
    response: 'ã¾ãŸã­ï¼æ¥½ã—ã‹ã£ãŸã§ã™ï¼',
    feedback: 'ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªåˆ¥ã‚Œã®æŒ¨æ‹¶ã§ã™ï¼å®Œç’§ã§ã™ã€‚'
  }],
]);

// OPTIMIZED VERSION - Faster response with streaming internally
router.post(
  '/api/generate-response',
  verifyFirebaseToken,
  async (req: Request, res: Response<ResponseData | { error: string }>) => {
    const startTime = Date.now();
    console.log('âš¡ [LLM-OPTIMIZED] Response generation request received (streaming internally)');

    try {
      const { speech, jlptLevel, grammarPrompt } = req.body as { speech?: string; jlptLevel?: string; grammarPrompt?: string };

      console.log(`ğŸ“ [LLM-SERVER] User input: "${speech?.substring(0, 100)}"`);
      console.log(`ğŸ“Š [LLM-SERVER] JLPT Level: ${jlptLevel || 'none'}, Grammar: ${grammarPrompt || 'none'}`);

      if (!speech || speech.trim().length === 0) {
        console.error('âŒ [LLM-SERVER] Empty speech provided');
        return res.status(400).json({
          error: 'Invalid request. Provide "speech" in the body.',
        });
      }

      // OPTIMIZATION: Check cache first for instant responses!
      const normalizedSpeech = speech.trim().toLowerCase();
      const cached = responseCache.get(normalizedSpeech);
      if (cached) {
        console.log(`âš¡ [CACHE-HIT] Instant response for: "${speech}"`);
        console.log(`â±ï¸ [CACHE-HIT] Total time: ${Date.now() - startTime}ms (INSTANT!)`);
        return res.status(200).json(cached);
      }
      console.log('ğŸ“ [CACHE-MISS] Not in cache, generating response...');

      // OPTIMIZATION: Use structured text format instead of JSON to enable streaming in future
      let systemPrompt =
        'Your name is Tanuki Chan and you are a friendly native Japanese speaker helping a learner. ' +
        'Analyse their speech, give concise grammar feedback, then reply and ' +
        'encourage them to continue in Japanese.\n\n' +
        'Format your response EXACTLY as follows:\n' +
        'RESPONSE: [Your Japanese reply to the user]\n' +
        'FEEDBACK: [Your concise grammar feedback]';

        if (jlptLevel) {
        systemPrompt += `\n\nRespond ONLY at JLPT level ${jlptLevel}.`;
      }
      if (grammarPrompt) {
        systemPrompt += `\n\nCenter your reply around practicing the following grammar point: "${grammarPrompt}". ` +
                        `Make your response prompt the user to use and notice this grammar. Also, if the user's message did not use this grammar yet, encourage them to try.`;
      }

      const llmStart = Date.now();
      console.log('ğŸ“¤ [LLM-OPTIMIZED] Sending to OpenAI with streaming...');

      // OPTIMIZATION: Use lower temperature for simple inputs (faster generation)
      const isSimpleInput = speech.length < 20;
      const temperature = isSimpleInput ? 0.5 : 0.7;
      if (isSimpleInput) {
        console.log('âš¡ [LLM-OPTIMIZED] Simple input detected, using temperature 0.5 for faster generation');
      }

      // Use streaming internally for faster response
      const stream = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: speech },
        ],
        temperature,
        stream: true,  // Stream internally to get first sentence faster
      });

      let fullContent = '';
      let firstSentence = '';
      let firstSentenceTime: number | null = null;

      // Stream and detect first sentence
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        fullContent += content;

        // Detect first sentence (for internal optimization)
        if (!firstSentenceTime && /[ã€‚ï¼ï¼Ÿ]/.test(fullContent)) {
          const match = fullContent.match(/RESPONSE:\s*(.+?[ã€‚ï¼ï¼Ÿ])/);
          if (match) {
            firstSentence = match[1].trim();
            firstSentenceTime = Date.now();
            console.log(`ğŸ¯ [LLM-OPTIMIZED] First sentence ready in ${firstSentenceTime - llmStart}ms: "${firstSentence}"`);
          }
        }
      }

      console.log(`ğŸ“¥ [LLM-OPTIMIZED] Stream complete in ${Date.now() - llmStart}ms`);
      console.log(`ğŸ“ [LLM-OPTIMIZED] Full content: "${fullContent.substring(0, 150)}..."`);

      // Parse structured text response
      const responseMatch = fullContent.match(/RESPONSE:\s*([\s\S]+?)(?=\nFEEDBACK:|$)/);
      const feedbackMatch = fullContent.match(/FEEDBACK:\s*([\s\S]+?)$/);

      const parsed: ResponseData & { firstSentence?: string } = {
        response: responseMatch?.[1]?.trim() || 'ã“ã‚“ã«ã¡ã¯ï¼',
        feedback: feedbackMatch?.[1]?.trim() || 'Keep practicing!',
        firstSentence: firstSentence || undefined,  // Send first sentence separately
      };

      console.log(`âœ… [LLM-OPTIMIZED] Parsed response: "${parsed.response?.substring(0, 50)}..."`);
      console.log(`âœ… [LLM-OPTIMIZED] Parsed feedback: "${parsed.feedback?.substring(0, 50)}..."`);

      if (firstSentenceTime) {
        console.log(`âš¡ [LLM-OPTIMIZED] Time saved by detecting first sentence: ${(Date.now() - firstSentenceTime)}ms`);
      }

      // Validate with schema
      responseSchema.parse(parsed);

      console.log(`â±ï¸ [LLM-OPTIMIZED] Total time: ${Date.now() - startTime}ms`);
      return res.status(200).json(parsed);
    } catch (err: any) {
      console.error('âŒ [LLM-SERVER] Error:', err);
      console.error('âŒ [LLM-SERVER] Error details:', err?.message, err?.response?.data);

      if (err.status === 429 || err.code === 'insufficient_quota') {
        return res
          .status(429)
          .json({ error: 'OpenAI quota exceeded. Check billing.' });
      }

      return res.status(500).json({ error: 'Failed to generate response.' });
    }
  }
);

// STREAMING VERSION - New endpoint for streaming responses
router.post(
  '/api/generate-response-stream',
  verifyFirebaseToken,
  async (req: Request, res: Response) => {
    const startTime = Date.now();
    console.log('ğŸŒŠ [LLM-STREAM] Streaming response generation started');

    try {
      const { speech, jlptLevel, grammarPrompt } = req.body as { speech?: string; jlptLevel?: string; grammarPrompt?: string };

      console.log(`ğŸ“ [LLM-STREAM] User input: "${speech?.substring(0, 100)}"`);

      if (!speech || speech.trim().length === 0) {
        return res.status(400).json({ error: 'Invalid request. Provide "speech" in the body.' });
      }

      // Set up Server-Sent Events
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      let systemPrompt =
        'Your name is Tanuki Chan and you are a friendly native Japanese speaker helping a learner. ' +
        'Analyse their speech, give concise grammar feedback, then reply and ' +
        'encourage them to continue in Japanese.\n\n' +
        'Format your response EXACTLY as follows:\n' +
        'RESPONSE: [Your Japanese reply to the user]\n' +
        'FEEDBACK: [Your concise grammar feedback]';

      if (jlptLevel) {
        systemPrompt += `\n\nRespond ONLY at JLPT level ${jlptLevel}.`;
      }
      if (grammarPrompt) {
        systemPrompt += `\n\nCenter your reply around practicing the following grammar point: "${grammarPrompt}". ` +
                        `Make your response prompt the user to use and notice this grammar. Also, if the user's message did not use this grammar yet, encourage them to try.`;
      }

      const llmStart = Date.now();
      console.log('ğŸ“¤ [LLM-STREAM] Starting streaming from OpenAI...');

      const stream = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: speech },
        ],
        temperature: 0.7,
        stream: true,  // Enable streaming!
      });

      let fullContent = '';
      let buffer = '';
      let inResponseSection = false;
      let responseSent = false;
      let firstSentenceTime: number | null = null;

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        if (!content) continue;

        fullContent += content;
        buffer += content;

        // Detect when we're in the RESPONSE section
        if (buffer.includes('RESPONSE:')) {
          inResponseSection = true;
          buffer = buffer.split('RESPONSE:')[1] || '';
        }

        // Detect sentence boundaries in Japanese (ã€‚ï¼ï¼Ÿ)
        if (inResponseSection && !responseSent && /[ã€‚ï¼ï¼Ÿ]/.test(buffer)) {
          // Extract first sentence
          const sentenceMatch = buffer.match(/^(.+?[ã€‚ï¼ï¼Ÿ])/);
          if (sentenceMatch) {
            const firstSentence = sentenceMatch[1].trim();
            firstSentenceTime = Date.now();

            console.log(`ğŸ¯ [LLM-STREAM] First sentence ready in ${firstSentenceTime - llmStart}ms: "${firstSentence}"`);

            // Send first sentence to frontend immediately!
            res.write(`data: ${JSON.stringify({ type: 'first_sentence', content: firstSentence })}\n\n`);
            responseSent = true;
          }
        }

        // Check if we've moved to FEEDBACK section
        if (buffer.includes('FEEDBACK:')) {
          inResponseSection = false;
        }
      }

      console.log(`ğŸ“¥ [LLM-STREAM] Stream complete in ${Date.now() - llmStart}ms`);
      console.log(`ğŸ“ [LLM-STREAM] Full content: "${fullContent.substring(0, 150)}..."`);

      // Parse complete response
      const responseMatch = fullContent.match(/RESPONSE:\s*([\s\S]+?)(?=\nFEEDBACK:|$)/);
      const feedbackMatch = fullContent.match(/FEEDBACK:\s*([\s\S]+?)$/);

      const parsed = {
        response: responseMatch?.[1]?.trim() || 'ã“ã‚“ã«ã¡ã¯ï¼',
        feedback: feedbackMatch?.[1]?.trim() || 'Keep practicing!',
      };

      console.log(`âœ… [LLM-STREAM] Complete response: "${parsed.response?.substring(0, 50)}..."`);
      console.log(`âœ… [LLM-STREAM] Feedback: "${parsed.feedback?.substring(0, 50)}..."`);

      // Send complete response
      res.write(`data: ${JSON.stringify({ type: 'complete', ...parsed })}\n\n`);
      res.write('data: [DONE]\n\n');
      res.end();

      console.log(`â±ï¸ [LLM-STREAM] Total time: ${Date.now() - startTime}ms`);
      if (firstSentenceTime) {
        console.log(`âš¡ [LLM-STREAM] Time saved: ${(Date.now() - firstSentenceTime)}ms (TTS started early!)`);
      }

    } catch (err: any) {
      console.error('âŒ [LLM-STREAM] Error:', err);
      res.write(`data: ${JSON.stringify({ type: 'error', error: err.message })}\n\n`);
      res.end();
    }
  }
);

export default router;
